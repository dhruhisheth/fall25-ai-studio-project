{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/BTT-Cadence-Design-Systems-2A/AI-Studio-Project/blob/Bert-base-multilingual-uncased-sentiment/Cadence_2A_Bert_base_cased_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0Py6PRMeBIM"
   },
   "source": [
    " **Install libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkPWATdBrLTM",
    "outputId": "844e92ed-4cb3-4842-f600-8c839a67e27e"
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0kQN_bwNx3u"
   },
   "source": [
    "**Imports & config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KH6pkdTmN10E",
    "outputId": "fda9ab22-5185-4fe4-b064-151ceb005474"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import fsspec\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "REPO = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "\n",
    "\n",
    "CATEGORIES = [\"Software\", \"Video_Games\", \"All_Beauty\"]\n",
    "ALL_CATEGORIES = [\"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\", \"Automotive\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Books\",\n",
    "              \"CDs_and_Vinyl\", \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewelry\", \"Digital_Music\", \"Electronics\", \"Gift_Cards\", \"Grocery_and_Gourmet_Food\",\n",
    "              \"Handmade_Products\", \"Health_and_Household\", \"Health_and_Personal_Care\", \"Home_and_Kitchen\", \"Industrial_and_Scientific\",\n",
    "              \"Kindle_Store\", \"Magazine_Subscriptions\", \"Movies_and_TV\", \"Musical_Instruments\", \"Office_Products\", \"Patio_Lawn_and_Garden\", \"Pet_Supplies\",\n",
    "              \"Software\", \"Sports_and_Outdoors\", \"Subscription_Boxes\", \"Tools_and_Home_Improvement\", \"Toys_and_Games\", \"Video_Games\",\n",
    "              \"Unknown\"]\n",
    "\n",
    "\n",
    "N_PER_CAT = 10_000\n",
    "N_META    = 60_000\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYW6dqUGeUMR"
   },
   "source": [
    "**Load & sample each category (streaming) and concatenate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywISq46crPoL"
   },
   "outputs": [],
   "source": [
    "def stream_jsonl(url: str, limit: int | None = None):\n",
    "    \"\"\"\n",
    "    Stream a JSONL file line-by-line from Hugging Face\n",
    "    Normalizes mixed-type fields like 'price'\n",
    "    \"\"\"\n",
    "    with fsspec.open(url, \"rt\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if limit is not None and idx >= limit:\n",
    "                break\n",
    "            obj = json.loads(line)\n",
    "\n",
    "\n",
    "            if \"price\" in obj and obj[\"price\"] is not None:\n",
    "                obj[\"price\"] = str(obj[\"price\"])\n",
    "\n",
    "            return_obj = obj\n",
    "            yield return_obj\n",
    "\n",
    "\n",
    "def ensure_asin(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure there is an 'asin' column\n",
    "    \"\"\"\n",
    "    for cand in [\"asin\", \"parent_asin\", \"product_id\", \"item_id\", \"Parent_ASIN\", \"ParentAsin\"]:\n",
    "        if cand in df.columns:\n",
    "            if \"asin\" not in df.columns:\n",
    "                df[\"asin\"] = df[cand]\n",
    "            return df\n",
    "    if len(df) > 0:\n",
    "        print(\"No recognizable ASIN-like key found. Example row:\\n\", df.head(1).to_dict(\"records\")[0])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_category(category: str, n_reviews: int, n_meta: int):\n",
    "    \"\"\"\n",
    "    Load one category's reviews and meta as DataFrames\n",
    "    \"\"\"\n",
    "    reviews_url = f\"hf://datasets/{REPO}/raw/review_categories/{category}.jsonl\"\n",
    "    meta_url    = f\"hf://datasets/{REPO}/raw/meta_categories/meta_{category}.jsonl\"\n",
    "\n",
    "    reviews_df = pd.DataFrame(islice(stream_jsonl(reviews_url), n_reviews)).assign(category=category)\n",
    "    meta_df    = pd.DataFrame(islice(stream_jsonl(meta_url),    n_meta)).assign(category=category)\n",
    "    return reviews_df, meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMKbJ5SBerZk"
   },
   "source": [
    "**Inspect schemas and key columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zWG-GjTrrMgT",
    "outputId": "4217f246-75f0-46bc-a9bf-eb79f928f710"
   },
   "outputs": [],
   "source": [
    "all_reviews, all_meta = [], []\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    r_df, m_df = load_category(cat, n_reviews=N_PER_CAT, n_meta=N_META)\n",
    "    all_reviews.append(r_df)\n",
    "    all_meta.append(m_df)\n",
    "\n",
    "reviews_df = pd.concat(all_reviews, ignore_index=True)\n",
    "meta_df    = pd.concat(all_meta,    ignore_index=True)\n",
    "\n",
    "reviews_df = ensure_asin(reviews_df)\n",
    "meta_df    = ensure_asin(meta_df)\n",
    "\n",
    "\n",
    "if \"asin\" in reviews_df:\n",
    "    reviews_df = reviews_df[reviews_df[\"asin\"].notna()]\n",
    "if \"asin\" in meta_df:\n",
    "    meta_df = meta_df[meta_df[\"asin\"].notna()]\n",
    "\n",
    "print(f\"Loaded rows -> reviews: {len(reviews_df):,} | meta: {len(meta_df):,}\")\n",
    "display(reviews_df.head(2))\n",
    "display(meta_df.head(2))\n",
    "\n",
    "print(f\"Unique products in reviews: {reviews_df['asin'].nunique():,}\")\n",
    "print(f\"Unique products in meta: {meta_df['asin'].nunique():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRx0vu7lebCQ"
   },
   "source": [
    "**Helper: ensure_asin + normalize IDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tOMpYsb9rYi2",
    "outputId": "5ee690f3-7d37-414f-cc68-60e7cc3b7f26"
   },
   "outputs": [],
   "source": [
    "meta_keys = {\"asin\", \"parent_asin\", \"category\"}\n",
    "meta_keep = [\"asin\", \"parent_asin\"] + [c for c in meta_df.columns if c not in meta_keys]\n",
    "\n",
    "\n",
    "m1 = reviews_df.merge(meta_df[meta_keep], on=\"asin\", how=\"left\", suffixes=(\"_review\", \"_meta\"))\n",
    "\n",
    "\n",
    "m2 = reviews_df.merge(\n",
    "    meta_df[meta_keep].rename(columns={\"asin\": \"asin_meta2\", \"parent_asin\": \"parent_asin_meta2\"}),\n",
    "    left_on=\"parent_asin\",\n",
    "    right_on=\"asin_meta2\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "\n",
    "merged = m1.copy()\n",
    "for col in meta_keep:\n",
    "    if col in {\"asin\", \"parent_asin\"}:\n",
    "        continue\n",
    "    col_m1 = col\n",
    "    col_m2 = col + \"_m2\"\n",
    "    if col in m2.columns:\n",
    "        merged[col_m2] = m2[col]\n",
    "        merged[col] = merged[col].where(merged[col].notna(), merged[col_m2])\n",
    "        merged.drop(columns=[col_m2], inplace=True)\n",
    "\n",
    "\n",
    "if \"asin_meta2\" in m2.columns:\n",
    "    merged[\"asin_meta_fallback\"] = m2[\"asin_meta2\"]\n",
    "\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "\n",
    "\n",
    "meta_signal = [c for c in merged.columns if c.endswith(\"_meta\") or c in [\"average_rating\", \"rating_number\", \"price\", \"store\", \"categories\", \"details\", \"title\", \"images\", \"videos\", \"main_category\"]]\n",
    "coverage = merged[meta_signal].notna().any(axis=1).mean() if meta_signal else 0.0\n",
    "print(f\"Rows with ANY meta fields: {coverage:.2%}\")\n",
    "\n",
    "display(merged.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fgmk-O7tD_NL"
   },
   "source": [
    "# **Milestone #1: Sentiment Analysis of a Singular Review**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RvtkE9vEWtv"
   },
   "source": [
    "Goal: Take the reviews dataframe, only maintain the rating, title, category, and text columns, and then train a model that predicts the rating given a review text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ml7wXwvfEpAi"
   },
   "outputs": [],
   "source": [
    "def load_category_into_review(category: str, n_reviews: int):\n",
    "    \"\"\"\n",
    "    Load one category's reviews as DataFrames\n",
    "    \"\"\"\n",
    "    reviews_url = f\"hf://datasets/{REPO}/raw/review_categories/{category}.jsonl\"\n",
    "\n",
    "    data = (\n",
    "        {k: row.get(k) for k in [\"rating\", \"title\", \"text\"]}\n",
    "        for row in islice(stream_jsonl(reviews_url), n_reviews)\n",
    "    )\n",
    "\n",
    "    reviews_df = pd.DataFrame(data).assign(category=category)\n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "cJDeGPbQVOUP",
    "outputId": "fc755d1d-7ecc-427e-8e77-2c6737da2799"
   },
   "outputs": [],
   "source": [
    "\n",
    "sentiment_reviews =  []\n",
    "\n",
    "for cat in ALL_CATEGORIES:\n",
    "    r_df = load_category_into_review(cat, n_reviews=N_PER_CAT)\n",
    "    sentiment_reviews.append(r_df)\n",
    "\n",
    "reviews_df_milestone1 = pd.concat(sentiment_reviews, ignore_index=True)\n",
    "\n",
    "\n",
    "print(f\"Loaded rows -> reviews: {len(reviews_df_milestone1):,}\")\n",
    "display(reviews_df_milestone1.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "6r_dO7urHZ78",
    "outputId": "f0a6e828-d23e-4724-d6ed-22142a122553"
   },
   "outputs": [],
   "source": [
    "reviews_df_milestone1.info()\n",
    "reviews_df_milestone1['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHKrG0joIYVJ"
   },
   "source": [
    "## Milestone #1: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "-I-iBiBqHajJ",
    "outputId": "f2b9e595-268d-4bc6-b034-54e7bc12dbd2"
   },
   "outputs": [],
   "source": [
    "reviews_df_milestone1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZ8x0--IT4qv"
   },
   "source": [
    "### Text Normalization (removing punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j30V-EbkT2TO"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Function removes all punctuation from a string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6meTumTwIeFC"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Creates clean_review and clean_title and clean_review. These two columns will be used during model training.\n",
    "\"\"\"\n",
    "reviews_df_milestone1['clean_review'] = (\n",
    "    reviews_df_milestone1['text']\n",
    "    .str.lower()\n",
    "    .apply(remove_punctuation)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "reviews_df_milestone1['clean_title'] = (\n",
    "    reviews_df_milestone1['title']\n",
    "    .str.lower()\n",
    "    .apply(remove_punctuation)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdv3iTFJbE2B"
   },
   "source": [
    "### Lemmitization of Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STEnIP8cbujd"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EVpjl62cAEN"
   },
   "outputs": [],
   "source": [
    "reviews_df_milestone1['lemmatized_review'] = reviews_df_milestone1['clean_review'].apply(lemmatize_text)\n",
    "reviews_df_milestone1['lemmatized_title'] = reviews_df_milestone1['clean_title'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gD-wBOoYFyD"
   },
   "source": [
    "### Creating Sentiment Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hypuTaGqYK3y"
   },
   "outputs": [],
   "source": [
    "def create_sentiment_label(rating: int) -> str:\n",
    "  if rating >= 4:\n",
    "    return 'positive'\n",
    "  elif rating <= 2:\n",
    "    return 'negative'\n",
    "  else:\n",
    "    return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ4S2UrnYV-t"
   },
   "outputs": [],
   "source": [
    "reviews_df_milestone1['sentiment_labels'] = (\n",
    "    reviews_df_milestone1['rating']\n",
    "    .apply(create_sentiment_label)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "-tI5cmBlYeZn",
    "outputId": "447b20ab-9eb5-478b-c974-faad68ad56b8"
   },
   "outputs": [],
   "source": [
    "reviews_df_milestone1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIqjyAdYdxmz"
   },
   "source": [
    "### Tokenization of Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiY9sb8-dVpV"
   },
   "outputs": [],
   "source": [
    "# documents = reviews_df_milestone1['clean_review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiBi6Z1VdXeC"
   },
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(\n",
    "#     stop_words=\"english\",   # remove english stopwords like this, a, the, etc\n",
    "#     # max_features=5000,      # keep top 5000 words (tune this)\n",
    "# )\n",
    "# X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxObKv5odZUF"
   },
   "outputs": [],
   "source": [
    "# print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjPm7m-Xh6mH"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "reviews_df_milestone1['tokenized_review'] = reviews_df_milestone1['clean_review'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP5_RHwGiGMT"
   },
   "outputs": [],
   "source": [
    "reviews_df_milestone1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-OfLn5QJmB1"
   },
   "source": [
    "### Creating another column for sentiment label classes\n",
    "For this label, we will set 0 - negative, 1 - neutral, 2 - positive.We need this to calculate loss between model output and the ground truth sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y53J2YzNJ8u7"
   },
   "outputs": [],
   "source": [
    "def create_sentiment_label_classes(rating: int) -> str:\n",
    "  if rating >= 4:\n",
    "    return 2\n",
    "  elif rating <= 2:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V16I9HnlKIgm"
   },
   "outputs": [],
   "source": [
    "reviews_df_milestone1['sentiment_label_classes'] = (\n",
    "    reviews_df_milestone1['rating']\n",
    "    .apply(create_sentiment_label_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9hSWw0shKYD6",
    "outputId": "7a57a1df-d00f-4fb6-cc95-8aaf2f7f8a07"
   },
   "outputs": [],
   "source": [
    "reviews_df_milestone1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usj5lHibpypQ"
   },
   "source": [
    "# Bert-base Cased Model for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOCsHwR10ZxQ"
   },
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKKgaHV4j73x",
    "outputId": "3831a832-aa9e-429a-9f19-3940ba9ec301"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FMq91W1trFC"
   },
   "source": [
    "### Convert \"clean review\" column to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGaYlTmLt2Fg"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "clean_reviews_dataset = Dataset.from_pandas(reviews_df_milestone1[['clean_review', 'sentiment_label_classes']]).rename_column(\"sentiment_label_classes\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvCzmwKduYWP",
    "outputId": "b0714ef3-3f5b-4d99-85db-6564d03145ae"
   },
   "outputs": [],
   "source": [
    "print(clean_reviews_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZPjl3TZpmAn"
   },
   "source": [
    "### Load model and model's tokenizer to convert cleaned review text to number embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "73d6177fa192494aa3ee5efc71ffc092",
      "031a7e6849b744328937075f9241198a",
      "1acb28f3e57f4a94ac1ac8872620e8dd",
      "d6494c0060314e3790612da7dc891c10",
      "b2a599dab58e42c3a25c31de9dc04aa2",
      "dba7834d27934911ae1fdc498a43780a",
      "98fdded32ead4bbcbbda43e0a1ecbd6e",
      "2fd2ce93188a4f4d9014bdeb8977b07c",
      "2a90b89f6441428a8229b2136ff42c4e",
      "0e15be8e5ad74a90ad0b25f04621d82e",
      "8a974e064ad14e9cac6d5b920b298d54",
      "f9968ce59cdf4a078fe72973adad3b07",
      "b1cf5eb52b7e4d959dbadfdb34c17ab1",
      "5722030dd39743688c55b029fed400ce",
      "51ead3bbf1084581a7a1827eb25083af",
      "33593f42cb3b47e7b5fa66f1d774e270",
      "bb0829d9961e405ca6214fe9d24a8cb4",
      "be5fa3e577ca4224b9c7db4dd75e6769",
      "801ab196f10c460f83a0a84526c6279d",
      "d8f912e622054f49b5de8211e5bcffc9",
      "565d5ad414db4bd1af07b93178488831",
      "43a7aed5d19c4d2ebcb2ac863527b926",
      "835f1f5f130845df82cf7ffcf6954e85",
      "5b9c5273ccfd4484886eb4eda6e5d78f",
      "cda320b5a2d54221ba3d8fe78b4c1d18",
      "50749d937aa34e63bf93ab4f897639b9",
      "2bfe0c82af70445c979ad22323873f7a",
      "484c4f501ec840df88e13c8e58c3e113",
      "1285a09f097e4c57ac0f12ab9dca229b",
      "1fe2f3cbc20742689b67aef85f7b257a",
      "8a2ff7ec6055440c9d43e38ab795d5ad",
      "c8e733d7b2e94fada077d2a185694c41",
      "fd7f7c0d237c468e9432291047f4a1dc",
      "41a3dd68738b484a85fdfbbafb3fe8a9",
      "b420b10c87b748b5b0a9ff3fe383e906",
      "eecb3712982243a8b071797b7b5dcbbb",
      "2fbbc877f1c84d2cbdb90ac21e1a5e0c",
      "caf4494805ee4899aaf93c93d96ee0a4",
      "56f8be9d2efc47d38331dda94a3e77d4",
      "fdb99aefea2f4a23ba461fb090ad5c83",
      "eed28e2f13694e868a545da26470f7ef",
      "f9cb5b11e884456daaa9ae50113e06e6",
      "939c60a9fa1447f592bc8b57e6439bd9",
      "f1c8b5dedc3a4d839d5644cf81f9f5fe",
      "927df04733e349798a4c07383e784d16",
      "32a51038ef8648d0a339e83934a01973",
      "af935fe193ff45929e5a451d3d76ec48",
      "1912f6ad5b55476693cd5ff6dfd20ef8",
      "2e6c97dc1c69418d843170091a29de7b",
      "afc29720000c45868428a0432a9e6900",
      "6422b213b45e46619296c34a6bd411a1",
      "8f24a6af442a4728a3fff01f2063d920",
      "2ef0ea97204c41a394384f1adaee49ff",
      "c43891f0d2104a0baf337e352f2281f6",
      "5940720a23d24fcfaf5957220b58fc07"
     ]
    },
    "id": "E4qLOCWYk0Vl",
    "outputId": "ae7626a9-3739-4440-88bb-b6b928386d02"
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# define the tokenization function\n",
    "def tokenize_text(examples):\n",
    "  return tokenizer(examples['clean_review'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# apply tokenization func to the clean review text\n",
    "tokenized_clean_reviews = clean_reviews_dataset.map(tokenize_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFfXqjbKphqx",
    "outputId": "acf2f174-f08e-4a63-93b1-4545a18e371e"
   },
   "outputs": [],
   "source": [
    "print(tokenized_clean_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEaAPZIcq2-Y"
   },
   "source": [
    "### Split the tokenized text into training, validation, and test set\n",
    "We will use 70% training set, 15% validation set and 15% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-PDr9u8qpAa"
   },
   "outputs": [],
   "source": [
    "train_dataset, valid_test_dataset = tokenized_clean_reviews.train_test_split(test_size=0.3, seed=42).values()\n",
    "valid_dataset, test_dataset = valid_test_dataset.train_test_split(test_size=0.5, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWj52UONzTjv",
    "outputId": "b81711e3-35ec-4977-f403-74443c141c7c"
   },
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(f\"train dataset shape: {train_dataset.shape}\")\n",
    "print(valid_dataset)\n",
    "print(f\"valid dataset shape: {valid_dataset.shape}\")\n",
    "print(test_dataset)\n",
    "print(f\"test dataset shape: {test_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e_YAVegzunV"
   },
   "source": [
    "### Create data loader to manage batches of data during training\n",
    "Dataloader is used to organize data for model training by providing efficient ways to batch, shuffle, and transform data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECYRGIpVz3Yd"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Igszu8F1U2Q"
   },
   "source": [
    "### Setting up the model and config for fine-tuning the model\n",
    "\n",
    "AdamW is for adjusting learning rate during training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlabyXAk1gkA"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# load the pre-trained bert model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5MegwT4_mOd",
    "outputId": "98e0424f-89b7-4461-91af-05333a75596a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "LOlL5bIk25V1",
    "outputId": "cbc4aa15-afd2-4105-efd4-eff843f452b3"
   },
   "outputs": [],
   "source": [
    "# define training argument\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/Bert-base Multilingual Uncase Model checkpoint', # ouput directory for model checkpoint\n",
    "    eval_strategy='epoch', # evaluation strategy\n",
    "    num_train_epochs=3, # num of training epochs\n",
    "    learning_rate=2e-5, # learning rate\n",
    "    per_device_train_batch_size=8, # batch size for training\n",
    "    per_device_eval_batch_size=8, # batch size for evaluation\n",
    "    weight_decay=0.01 # weight decay\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBmVohGV4r8t"
   },
   "outputs": [],
   "source": [
    "# define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "3wLDhgpcEEfx",
    "outputId": "24f222ab-aeb2-4b45-968c-4cb3f841e7cf"
   },
   "outputs": [],
   "source": [
    "trainer.train() # Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxloMk-gPd9q"
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8w7QbvmVTjoq"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "3Bv8q-ncowlu",
    "outputId": "7d10519a-e482-4b02-e612-dd96f4d0d135"
   },
   "outputs": [],
   "source": [
    "res = trainer.predict(test_dataset)\n",
    "logits = res.predictions # pred scores for each class\n",
    "labels = res.label_ids # label for each class\n",
    "pred = logits.argmax(axis=-1)   # pick the class with highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aYWnCMXPp9X",
    "outputId": "5ee34953-9319-458f-eac3-cd7b89cd06dc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "acc = accuracy_score(labels, pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(labels, pred, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BlXqlDRSjDa"
   },
   "source": [
    "### Inference of the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "4237cc48",
    "outputId": "f583bbb5-1b97-4a62-ccd6-80dd8e7ffa36"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer\n",
    "\n",
    "'''\n",
    "SAVING THE TOKENIZER IN THE SAVED CHECKPOINT\n",
    "finetuned_model_path = \"/content/drive/MyDrive/Bert-base Multilingual Uncase Model checkpoint/checkpoint-89250\"\n",
    "\n",
    "## Weight and parameters of the pre-trained model's tokenizer never changed\n",
    "# reload original pretrained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# save tokenizer into your finetuned model folder\n",
    "tokenizer.save_pretrained(finetuned_model_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "0-8sw0zxgCeL",
    "outputId": "5f113de7-edb5-49b7-8622-27fa4c9e9840"
   },
   "outputs": [],
   "source": [
    "finetuned_model_path = \"/content/drive/MyDrive/Bert-base Multilingual Uncase Model checkpoint/checkpoint-89250\"\n",
    "\n",
    "# load the finetuned model\n",
    "model = BertForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "# load the tokenizer in the finetuned model\n",
    "tokenizer = BertTokenizer.from_pretrained(finetuned_model_path)\n",
    "\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "# Running prediction\n",
    "res = trainer.predict(test_dataset)\n",
    "logits = res.predictions # pred scores for each class\n",
    "labels = res.label_ids # label for each class\n",
    "pred = logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0Px7PbjYlu0",
    "outputId": "ee13b457-3659-4158-c804-02327fe285b7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# print(pred)\n",
    "acc = accuracy_score(labels, pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(labels, pred, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9VAg9-YhDde"
   },
   "source": [
    "### Comparison of Original Bert vs. Finetuned Bert model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "zitpuT4W9X8j",
    "outputId": "240f47dd-ad70-4140-9215-5effc3f0d673"
   },
   "outputs": [],
   "source": [
    "# running the original bert-base cased model on the test dataset\n",
    "from transformers import BertForSequenceClassification, Trainer\n",
    "num_labels = 3  # adjust to your dataset\n",
    "\n",
    "orig_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "orig_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "orig_trainer = Trainer(model=orig_model)\n",
    "res = orig_trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVZtYjo1sCEl"
   },
   "outputs": [],
   "source": [
    "logits = res.predictions # pred scores for each class\n",
    "labels = res.label_ids # label for each class\n",
    "orig_model_pred = logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2528b48e",
    "outputId": "20d66660-1c79-4862-d6e6-196677ca7387"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# print(pred)\n",
    "orig_acc = accuracy_score(labels, orig_model_pred)\n",
    "orig_prec, orig_rec, orig_f1, _ = precision_recall_fscore_support(labels, orig_model_pred, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy: \", orig_acc)\n",
    "print(\"Precision:\", orig_prec)\n",
    "print(\"Recall:\", orig_rec)\n",
    "print(\"F1 score:\", orig_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrYgMbLksQR-"
   },
   "source": [
    "### Visualization of the accuracy of Original vs. Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "xklKYnYnh7yM",
    "outputId": "763fd916-adca-4ab5-f273-87708eb06c72"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [\"Accuracy\", \"F1 Score\"]\n",
    "orig_values = [orig_acc, orig_f1]\n",
    "finetuned_values = [acc, f1]\n",
    "\n",
    "x = range(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], orig_values, width=width, label=\"Original (Pretrained)\")\n",
    "plt.bar([i + width/2 for i in x], finetuned_values, width=width, label=\"Fine-tuned\")\n",
    "\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Original Bert-base cased vs Fine-tuned Performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cJ5S9Mgj4h4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
