{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BTT-Cadence-Design-Systems-2A/AI-Studio-Project/blob/Bert-base-cased-model/Cadence_2A_Bert_base_cased_model(ABSA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Py6PRMeBIM"
      },
      "source": [
        " **Install libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkPWATdBrLTM"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0kQN_bwNx3u"
      },
      "source": [
        "**Imports & config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH6pkdTmN10E"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import fsspec\n",
        "from itertools import islice\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "REPO = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
        "\n",
        "\n",
        "CATEGORIES = [\"Software\", \"Video_Games\", \"All_Beauty\"]\n",
        "ALL_CATEGORIES = [\"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\", \"Automotive\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Books\",\n",
        "              \"CDs_and_Vinyl\", \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewelry\", \"Digital_Music\", \"Electronics\", \"Gift_Cards\", \"Grocery_and_Gourmet_Food\",\n",
        "              \"Handmade_Products\", \"Health_and_Household\", \"Health_and_Personal_Care\", \"Home_and_Kitchen\", \"Industrial_and_Scientific\",\n",
        "              \"Kindle_Store\", \"Magazine_Subscriptions\", \"Movies_and_TV\", \"Musical_Instruments\", \"Office_Products\", \"Patio_Lawn_and_Garden\", \"Pet_Supplies\",\n",
        "              \"Software\", \"Sports_and_Outdoors\", \"Subscription_Boxes\", \"Tools_and_Home_Improvement\", \"Toys_and_Games\", \"Video_Games\",\n",
        "              \"Unknown\"]\n",
        "\n",
        "\n",
        "N_PER_CAT = 10_000\n",
        "N_META    = 60_000\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYW6dqUGeUMR"
      },
      "source": [
        "**Load & sample each category (streaming) and concatenate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywISq46crPoL"
      },
      "outputs": [],
      "source": [
        "def stream_jsonl(url: str, limit: int | None = None):\n",
        "    \"\"\"\n",
        "    Stream a JSONL file line-by-line from Hugging Face\n",
        "    Normalizes mixed-type fields like 'price'\n",
        "    \"\"\"\n",
        "    with fsspec.open(url, \"rt\") as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            if limit is not None and idx >= limit:\n",
        "                break\n",
        "            obj = json.loads(line)\n",
        "\n",
        "\n",
        "            if \"price\" in obj and obj[\"price\"] is not None:\n",
        "                obj[\"price\"] = str(obj[\"price\"])\n",
        "\n",
        "            return_obj = obj\n",
        "            yield return_obj\n",
        "\n",
        "\n",
        "def ensure_asin(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ensure there is an 'asin' column\n",
        "    \"\"\"\n",
        "    for cand in [\"asin\", \"parent_asin\", \"product_id\", \"item_id\", \"Parent_ASIN\", \"ParentAsin\"]:\n",
        "        if cand in df.columns:\n",
        "            if \"asin\" not in df.columns:\n",
        "                df[\"asin\"] = df[cand]\n",
        "            return df\n",
        "    if len(df) > 0:\n",
        "        print(\"No recognizable ASIN-like key found. Example row:\\n\", df.head(1).to_dict(\"records\")[0])\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_category(category: str, n_reviews: int, n_meta: int):\n",
        "    \"\"\"\n",
        "    Load one category's reviews and meta as DataFrames\n",
        "    \"\"\"\n",
        "    reviews_url = f\"hf://datasets/{REPO}/raw/review_categories/{category}.jsonl\"\n",
        "    meta_url    = f\"hf://datasets/{REPO}/raw/meta_categories/meta_{category}.jsonl\"\n",
        "\n",
        "    reviews_df = pd.DataFrame(islice(stream_jsonl(reviews_url), n_reviews)).assign(category=category)\n",
        "    meta_df    = pd.DataFrame(islice(stream_jsonl(meta_url),    n_meta)).assign(category=category)\n",
        "    return reviews_df, meta_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMKbJ5SBerZk"
      },
      "source": [
        "**Inspect schemas and key columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWG-GjTrrMgT"
      },
      "outputs": [],
      "source": [
        "all_reviews, all_meta = [], []\n",
        "\n",
        "for cat in CATEGORIES:\n",
        "    r_df, m_df = load_category(cat, n_reviews=N_PER_CAT, n_meta=N_META)\n",
        "    all_reviews.append(r_df)\n",
        "    all_meta.append(m_df)\n",
        "\n",
        "reviews_df = pd.concat(all_reviews, ignore_index=True)\n",
        "meta_df    = pd.concat(all_meta,    ignore_index=True)\n",
        "\n",
        "reviews_df = ensure_asin(reviews_df)\n",
        "meta_df    = ensure_asin(meta_df)\n",
        "\n",
        "\n",
        "if \"asin\" in reviews_df:\n",
        "    reviews_df = reviews_df[reviews_df[\"asin\"].notna()]\n",
        "if \"asin\" in meta_df:\n",
        "    meta_df = meta_df[meta_df[\"asin\"].notna()]\n",
        "\n",
        "print(f\"Loaded rows -> reviews: {len(reviews_df):,} | meta: {len(meta_df):,}\")\n",
        "display(reviews_df.head(2))\n",
        "display(meta_df.head(2))\n",
        "\n",
        "print(f\"Unique products in reviews: {reviews_df['asin'].nunique():,}\")\n",
        "print(f\"Unique products in meta: {meta_df['asin'].nunique():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df.columns"
      ],
      "metadata": {
        "id": "RCieP3Aql17V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRx0vu7lebCQ"
      },
      "source": [
        "**Helper: ensure_asin + normalize IDs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOMpYsb9rYi2"
      },
      "outputs": [],
      "source": [
        "meta_keys = {\"asin\", \"parent_asin\", \"category\", \"title\"}\n",
        "meta_keep = [\"asin\", \"parent_asin\", \"title\"] + [c for c in meta_df.columns if c not in meta_keys]\n",
        "\n",
        "\n",
        "m1 = reviews_df.merge(meta_df[meta_keep], on=\"asin\", how=\"left\", suffixes=(\"_review\", \"_meta\"))\n",
        "display(m1)\n",
        "m1.columns\n",
        "\n",
        "# m2 = reviews_df.merge(\n",
        "#     meta_df[meta_keep].rename(columns={\"asin\": \"asin_meta2\", \"parent_asin\": \"parent_asin_meta2\"}),\n",
        "#     left_on=\"parent_asin\",\n",
        "#     right_on=\"asin_meta2\",\n",
        "#     how=\"left\",\n",
        "# )\n",
        "\n",
        "\n",
        "# merged = m1.copy()\n",
        "# for col in meta_keep:\n",
        "#     if col in {\"asin\", \"parent_asin\"}:\n",
        "#         continue\n",
        "#     col_m1 = col\n",
        "#     col_m2 = col + \"_m2\"\n",
        "#     if col in m2.columns:\n",
        "#         merged[col_m2] = m2[col]\n",
        "#         merged[col] = merged[col].where(merged[col].notna(), merged[col_m2])\n",
        "#         merged.drop(columns=[col_m2], inplace=True)\n",
        "\n",
        "\n",
        "# if \"asin_meta2\" in m2.columns:\n",
        "#     merged[\"asin_meta_fallback\"] = m2[\"asin_meta2\"]\n",
        "\n",
        "# print(\"Merged shape:\", merged.shape)\n",
        "\n",
        "\n",
        "# meta_signal = [c for c in merged.columns if c.endswith(\"_meta\") or c in [\"average_rating\", \"rating_number\", \"price\", \"store\", \"categories\", \"details\", \"title\", \"images\", \"videos\", \"main_category\"]]\n",
        "# coverage = merged[meta_signal].notna().any(axis=1).mean() if meta_signal else 0.0\n",
        "# print(f\"Rows with ANY meta fields: {coverage:.2%}\")\n",
        "\n",
        "# display(merged.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eTky1292mSvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgmk-O7tD_NL"
      },
      "source": [
        "# **Milestone #1: Sentiment Analysis of a Singular Review**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RvtkE9vEWtv"
      },
      "source": [
        "Goal: Take the reviews dataframe, only maintain the rating, title, category, and text columns, and then train a model that predicts the rating given a review text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml7wXwvfEpAi"
      },
      "outputs": [],
      "source": [
        "# loading necessary column of review dataset\n",
        "def load_category_into_review(category: str, n_reviews: int):\n",
        "    \"\"\"\n",
        "    Load one category's reviews as DataFrames\n",
        "    \"\"\"\n",
        "    reviews_url = f\"hf://datasets/{REPO}/raw/review_categories/{category}.jsonl\"\n",
        "\n",
        "    data = (\n",
        "        {k: row.get(k) for k in [\"rating\", \"title\", \"text\", \"parent_asin\"]}\n",
        "        for row in islice(stream_jsonl(reviews_url), n_reviews)\n",
        "    )\n",
        "\n",
        "    reviews_df = pd.DataFrame(data).assign(category=category)\n",
        "    return reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_meta_data_into_review(category: str, m_reviews:int):\n",
        "#   '''\n",
        "#   Load one category's meta data as DataFrames\n",
        "\n",
        "#   '''\n",
        "#   meta_url = f\"hf://datasets/{REPO}/raw/meta_categories/meta_{category}.jsonl\"\n",
        "#   data = (\n",
        "#       {k: row.get(k) for k in [\"parent_asin\", \"title\"]}\n",
        "#       for row in islice(stream_jsonl(meta_url), m_reviews)\n",
        "#   )\n",
        "#   meta_df = pd.DataFrame(data).assign(category=category)\n",
        "#   return meta_df\n"
      ],
      "metadata": {
        "id": "rSSLLFmnusM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJDeGPbQVOUP"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentiment_reviews =  []\n",
        "\n",
        "for cat in ALL_CATEGORIES:\n",
        "    r_df = load_category_into_review(cat, n_reviews=N_PER_CAT) # review data\n",
        "    # m_df = load_meta_data_into_review(cat, m_reviews=N_META) # meta data\n",
        "    sentiment_reviews.append(r_df)\n",
        "    # print(f\"{cat} is appended\")\n",
        "\n",
        "reviews_df_milestone1 = pd.concat(sentiment_reviews, ignore_index=True)\n",
        "\n",
        "\n",
        "print(f\"Loaded rows -> reviews: {len(reviews_df_milestone1):,}\")\n",
        "display(reviews_df_milestone1.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r_dO7urHZ78"
      },
      "outputs": [],
      "source": [
        "reviews_df_milestone1.info()\n",
        "reviews_df_milestone1['rating'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHKrG0joIYVJ"
      },
      "source": [
        "## Milestone #1: Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I-iBiBqHajJ"
      },
      "outputs": [],
      "source": [
        "reviews_df_milestone1.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ8x0--IT4qv"
      },
      "source": [
        "### Text Normalization (removing punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j30V-EbkT2TO"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "\n",
        "def remove_punctuation(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Function removes all punctuation from a string\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6meTumTwIeFC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "   Creates clean_review and clean_title and clean_review. These two columns will be used during model training.\n",
        "\"\"\"\n",
        "reviews_df_milestone1['clean_review'] = (\n",
        "    reviews_df_milestone1['text']\n",
        "    .str.lower()\n",
        "    .apply(remove_punctuation)\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "reviews_df_milestone1['clean_title'] = (\n",
        "    reviews_df_milestone1['title']\n",
        "    .str.lower()\n",
        "    .apply(remove_punctuation)\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    .str.strip()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdv3iTFJbE2B"
      },
      "source": [
        "### Lemmitization of Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STEnIP8cbujd"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return \" \".join(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EVpjl62cAEN"
      },
      "outputs": [],
      "source": [
        "reviews_df_milestone1['lemmatized_review'] = reviews_df_milestone1['clean_review'].apply(lemmatize_text)\n",
        "reviews_df_milestone1['lemmatized_title'] = reviews_df_milestone1['clean_title'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gD-wBOoYFyD"
      },
      "source": [
        "### Creating Sentiment Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hypuTaGqYK3y"
      },
      "outputs": [],
      "source": [
        "def create_sentiment_label(rating: int) -> str:\n",
        "  if rating >= 4:\n",
        "    return 'positive'\n",
        "  elif rating <= 2:\n",
        "    return 'negative'\n",
        "  else:\n",
        "    return 'neutral'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ4S2UrnYV-t"
      },
      "outputs": [],
      "source": [
        "reviews_df_milestone1['sentiment_labels'] = (\n",
        "    reviews_df_milestone1['rating']\n",
        "    .apply(create_sentiment_label)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tI5cmBlYeZn"
      },
      "outputs": [],
      "source": [
        "reviews_df_milestone1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIqjyAdYdxmz"
      },
      "source": [
        "### Tokenization of Reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiY9sb8-dVpV"
      },
      "outputs": [],
      "source": [
        "# documents = reviews_df_milestone1['clean_review'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiBi6Z1VdXeC"
      },
      "outputs": [],
      "source": [
        "# vectorizer = TfidfVectorizer(\n",
        "#     stop_words=\"english\",   # remove english stopwords like this, a, the, etc\n",
        "#     # max_features=5000,      # keep top 5000 words (tune this)\n",
        "# )\n",
        "# X = vectorizer.fit_transform(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxObKv5odZUF"
      },
      "outputs": [],
      "source": [
        "# print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "\n",
        "# df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "# df_tfidf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjPm7m-Xh6mH"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "reviews_df_milestone1['tokenized_review'] = reviews_df_milestone1['clean_review'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP5_RHwGiGMT"
      },
      "outputs": [],
      "source": [
        "reviews_df_milestone1.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-OfLn5QJmB1"
      },
      "source": [
        "### Creating another column for sentiment label classes\n",
        "For this label, we will set 0 - negative, 1 - neutral, 2 - positive.We need this to calculate loss between model output and the ground truth sentiment labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y53J2YzNJ8u7"
      },
      "outputs": [],
      "source": [
        "def create_sentiment_label_classes(rating: int) -> str:\n",
        "  if rating >= 4:\n",
        "    return 2\n",
        "  elif rating <= 2:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V16I9HnlKIgm"
      },
      "outputs": [],
      "source": [
        "reviews_df_milestone1['sentiment_label_classes'] = (\n",
        "    reviews_df_milestone1['rating']\n",
        "    .apply(create_sentiment_label_classes)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hSWw0shKYD6"
      },
      "outputs": [],
      "source": [
        "reviews_df_milestone1.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usj5lHibpypQ"
      },
      "source": [
        "# Bert-base Cased Model for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOCsHwR10ZxQ"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKKgaHV4j73x"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FMq91W1trFC"
      },
      "source": [
        "### Convert \"clean review\" column to Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGaYlTmLt2Fg"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "clean_reviews_dataset = Dataset.from_pandas(reviews_df_milestone1[['clean_review', 'parent_asin','sentiment_label_classes']]).rename_column(\"sentiment_label_classes\", \"ground_truth_labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvCzmwKduYWP"
      },
      "outputs": [],
      "source": [
        "print(clean_reviews_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZPjl3TZpmAn"
      },
      "source": [
        "### Load model and model's tokenizer to convert cleaned review text to number embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4qLOCWYk0Vl"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# define the tokenization function\n",
        "def tokenize_text(examples):\n",
        "  return tokenizer(examples['clean_review'], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "# apply tokenization func to the clean review text\n",
        "tokenized_clean_reviews = clean_reviews_dataset.map(tokenize_text, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFfXqjbKphqx"
      },
      "outputs": [],
      "source": [
        "print(tokenized_clean_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEaAPZIcq2-Y"
      },
      "source": [
        "### Split the tokenized text into training, validation, and test set\n",
        "We will use 70% training set, 15% validation set and 15% test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-PDr9u8qpAa"
      },
      "outputs": [],
      "source": [
        "train_dataset, valid_test_dataset = tokenized_clean_reviews.train_test_split(test_size=0.3, seed=42).values()\n",
        "valid_dataset, test_dataset = valid_test_dataset.train_test_split(test_size=0.5, seed=42).values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWj52UONzTjv"
      },
      "outputs": [],
      "source": [
        "print(train_dataset)\n",
        "print(f\"train dataset shape: {train_dataset.shape}\")\n",
        "print(valid_dataset)\n",
        "print(f\"valid dataset shape: {valid_dataset.shape}\")\n",
        "print(test_dataset)\n",
        "print(f\"test dataset shape: {test_dataset.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e_YAVegzunV"
      },
      "source": [
        "### Create data loader to manage batches of data during training\n",
        "Dataloader is used to organize data for model training by providing efficient ways to batch, shuffle, and transform data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECYRGIpVz3Yd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=8)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Igszu8F1U2Q"
      },
      "source": [
        "### Setting up the model and config for fine-tuning the model\n",
        "\n",
        "AdamW is for adjusting learning rate during training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlabyXAk1gkA"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# load the pre-trained bert model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5MegwT4_mOd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOlL5bIk25V1"
      },
      "outputs": [],
      "source": [
        "# define training argument\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/Bert-base Multilingual Uncase Model checkpoint', # ouput directory for model checkpoint\n",
        "    eval_strategy='epoch', # evaluation strategy\n",
        "    num_train_epochs=3, # num of training epochs\n",
        "    learning_rate=2e-5, # learning rate\n",
        "    per_device_train_batch_size=8, # batch size for training\n",
        "    per_device_eval_batch_size=8, # batch size for evaluation\n",
        "    weight_decay=0.01 # weight decay\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBmVohGV4r8t"
      },
      "outputs": [],
      "source": [
        "# define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wLDhgpcEEfx"
      },
      "outputs": [],
      "source": [
        "trainer.train() # Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxloMk-gPd9q"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w7QbvmVTjoq"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Bv8q-ncowlu"
      },
      "outputs": [],
      "source": [
        "res = trainer.predict(test_dataset)\n",
        "logits = res.predictions # pred scores for each class\n",
        "labels = res.label_ids # label for each class\n",
        "pred = logits.argmax(axis=-1)   # pick the class with highest score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aYWnCMXPp9X"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "acc = accuracy_score(labels, pred)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(labels, pred, average=\"weighted\")\n",
        "\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", rec)\n",
        "print(\"F1 score:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BlXqlDRSjDa"
      },
      "source": [
        "### Inference of the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4237cc48"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer, Trainer\n",
        "\n",
        "'''\n",
        "SAVING THE TOKENIZER IN THE SAVED CHECKPOINT\n",
        "finetuned_model_path = \"/content/drive/MyDrive/Bert-base Multilingual Uncase Model checkpoint/checkpoint-89250\"\n",
        "\n",
        "## Weight and parameters of the pre-trained model's tokenizer never changed\n",
        "# reload original pretrained tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "\n",
        "# save tokenizer into your finetuned model folder\n",
        "tokenizer.save_pretrained(finetuned_model_path)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-8sw0zxgCeL"
      },
      "outputs": [],
      "source": [
        "finetuned_model_path = \"/content/drive/MyDrive/checkpoint-89250\" # change file paths based on where you store the model checkpoint\n",
        "\n",
        "# load the finetuned model\n",
        "model = BertForSequenceClassification.from_pretrained(finetuned_model_path)\n",
        "# load the tokenizer in the finetuned model\n",
        "tokenizer = BertTokenizer.from_pretrained(finetuned_model_path)\n",
        "\n",
        "trainer = Trainer(model=model)\n",
        "\n",
        "# Running prediction\n",
        "res = trainer.predict(test_dataset)\n",
        "logits = res.predictions # pred scores for each class\n",
        "labels = res.label_ids # label for each class\n",
        "pred = logits.argmax(axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Px7PbjYlu0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# print(pred)\n",
        "acc = accuracy_score(labels, pred)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(labels, pred, average=\"weighted\")\n",
        "\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", rec)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9VAg9-YhDde"
      },
      "source": [
        "### Comparison of Original Bert vs. Finetuned Bert model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zitpuT4W9X8j"
      },
      "outputs": [],
      "source": [
        "# running the original bert-base cased model on the test dataset\n",
        "from transformers import BertForSequenceClassification, Trainer\n",
        "num_labels = 3  # adjust to your dataset\n",
        "\n",
        "orig_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels=num_labels\n",
        ")\n",
        "orig_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "orig_trainer = Trainer(model=orig_model)\n",
        "res = orig_trainer.predict(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVZtYjo1sCEl"
      },
      "outputs": [],
      "source": [
        "logits = res.predictions # pred scores for each class\n",
        "labels = res.label_ids # label for each class\n",
        "orig_model_pred = logits.argmax(axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2528b48e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# print(pred)\n",
        "orig_acc = accuracy_score(labels, orig_model_pred)\n",
        "orig_prec, orig_rec, orig_f1, _ = precision_recall_fscore_support(labels, orig_model_pred, average=\"weighted\")\n",
        "\n",
        "print(\"Accuracy: \", orig_acc)\n",
        "print(\"Precision:\", orig_prec)\n",
        "print(\"Recall:\", orig_rec)\n",
        "print(\"F1 score:\", orig_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrYgMbLksQR-"
      },
      "source": [
        "### Visualization of the accuracy of Original vs. Finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xklKYnYnh7yM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = [\"Accuracy\", \"F1 Score\"]\n",
        "orig_values = [orig_acc, orig_f1]\n",
        "finetuned_values = [acc, f1]\n",
        "\n",
        "x = range(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar([i - width/2 for i in x], orig_values, width=width, label=\"Original (Pretrained)\")\n",
        "plt.bar([i + width/2 for i in x], finetuned_values, width=width, label=\"Fine-tuned\")\n",
        "\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.title(\"Original Bert-base cased vs Fine-tuned Performance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aspect Based Sentiment Analysis\n",
        "Aspect Based Sentiment Analysis (ABSA) is used to extract features that got good or bad reviews from customers to get insights about the good features and bad features of specific products.\n"
      ],
      "metadata": {
        "id": "Z5eE_x7j1J2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyabsa"
      ],
      "metadata": {
        "id": "JsdeqFnN-Uzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Aspect Extractor"
      ],
      "metadata": {
        "id": "-Hg1QT41HBzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyabsa import ATEPCCheckpointManager\n",
        "\n",
        "extractor = ATEPCCheckpointManager.get_aspect_extractor(\n",
        "    checkpoint='multilingual',\n",
        "    auto_device=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "myMabjRb4wny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Positive, Negative, and Neutral Aspects (Features) in the Reivew Products\n",
        "The features are extracted along with their corresponding sentiments in the product reveiws."
      ],
      "metadata": {
        "id": "syw1gw8VHGma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
        "import torch, torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "finetuned_model_path = \"/content/drive/MyDrive/checkpoint-89250\" # change file paths based on where you\n",
        "\n",
        "# load the finetuned model\n",
        "model = BertForSequenceClassification.from_pretrained(finetuned_model_path)\n",
        "# load the tokenizer in the finetuned model\n",
        "tokenizer = BertTokenizer.from_pretrained(finetuned_model_path)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"tmp\",\n",
        "    per_device_eval_batch_size=8\n",
        "\n",
        ")\n",
        "trainer = Trainer(model=model, args=args, tokenizer=tokenizer)\n",
        "\n",
        "# Running prediction\n",
        "res = trainer.predict(test_dataset)\n",
        "logits = res.predictions # pred scores for each class by the finetuned model\n",
        "pred = logits.argmax(axis=-1) # find the index of largest class and assign that class to review\n",
        "\n",
        "# probs and label strings\n",
        "probs = F.softmax(torch.tensor(logits), dim=-1)\n",
        "\n",
        "predicted_labels = [p for p in pred]\n"
      ],
      "metadata": {
        "id": "JAaW76msKQlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restructuring the dataset\n",
        "The test dataset is restructured into 'review', 'label', 'predicted label'. The positive and aspect features will be added to each review in the dataset."
      ],
      "metadata": {
        "id": "sB6gVvCKXI6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a dataset ['clean_review', 'ground truth labels']\n",
        "cols_keep = [\"clean_review\", \"ground truth labels\"]\n",
        "df_test = test_dataset.remove_columns(\n",
        "    [col for col in test_dataset.column_names if col not in cols_keep]\n",
        ")\n",
        "df_test = df_test.to_pandas()"
      ],
      "metadata": {
        "id": "xfyCu23cQlnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding predicted labels into the dataset\n",
        "df_test['predicted_labels'] = predicted_labels\n",
        "display(df_test.head(5))"
      ],
      "metadata": {
        "id": "8zqVpHEZO-dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the numerical labels to string labels\n",
        "idx2name = {0: \"negative\", 1: \"neutral\", 2:\"positive\"}\n",
        "df_test['predicted_labels'] = [idx2name[label] for label in df_test['predicted_labels']]\n",
        "df_test['ground truth labels'] = [idx2name[label] for label in df_test['ground truth labels']]"
      ],
      "metadata": {
        "id": "BlPJmrN6Y1xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test.columns)\n",
        "display(df_test.head(5))"
      ],
      "metadata": {
        "id": "lhH8M1QRZ-co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Positive, Negative, and Neutral features of the products in the dataset"
      ],
      "metadata": {
        "id": "XaI2pXlGXMLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = df_test[\"clean_review\"].tolist()\n",
        "results = extractor.extract_aspect(inference_source=reviews, pred_sentiment=True)\n",
        "\n",
        "n = len(reviews)\n",
        "good_aspects = [[] for _ in range(n)]\n",
        "bad_aspects = [[] for _ in range(n)]\n",
        "neutral_aspects = [[] for _ in range(n)]\n"
      ],
      "metadata": {
        "id": "vh94IBjCHITX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, r in enumerate(results):\n",
        "  aspects = r.get(\"aspect\") or r.get(\"aspects\") or []\n",
        "  sents = r.get(\"sentiment\") or r.get(\"sentiments\") or []\n",
        "\n",
        "  for a, s in zip(aspects, sents):\n",
        "    label = (str(s) or \"\").strip().lower()\n",
        "\n",
        "    if \"neg\" in label:\n",
        "      bad_aspects[i].append(a)\n",
        "    elif \"pos\" in label:\n",
        "      good_aspects[i].append(a)\n",
        "    elif \"neu\" in label or label == 'neutral':\n",
        "      neutral_aspects[i].append(a)\n",
        "    else:\n",
        "      neutral_aspects[i].append(a)"
      ],
      "metadata": {
        "id": "zX_WBZwPVIA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def empty_to_nan(lst):\n",
        "  return np.nan if not lst else lst\n",
        "\n",
        "df_test[\"Positive Review Product Features\"] = [empty_to_nan(x) for x in good_aspects]\n",
        "df_test[\"Negative Review Product Features\"] = [empty_to_nan(x) for x in bad_aspects]\n",
        "df_test[\"Neutral Review Product Features\"] = [empty_to_nan(x) for x in neutral_aspects]"
      ],
      "metadata": {
        "id": "6ErMuHb5k6I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.drop(\"Bad Review Product Features\", axis=1)\n",
        "display(df_test.head(5))"
      ],
      "metadata": {
        "id": "sTZvJdG4MIi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the dataset as csv file\n",
        "\n",
        "The reviews are saved in a csv file along with their ground truth labels (positive, negative, neutral based on ratings), predicted labels from Bert based cased model, extracted positive, negative, and neutral features.\n",
        "\n"
      ],
      "metadata": {
        "id": "yj2a0pPAMTJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_csv = \"/content/drive/MyDrive/Final_test_dataset.csv\"\n",
        "df_test.to_csv(output_csv, index=False)"
      ],
      "metadata": {
        "id": "ccPNtBqML9vh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}