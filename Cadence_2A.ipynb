{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BTT-Cadence-Design-Systems-2A/AI-Studio-Project/blob/twitter-roberta-base-sentiment/Cadence_2A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Install libraries**"
      ],
      "metadata": {
        "id": "a0Py6PRMeBIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XkPWATdBrLTM",
        "outputId": "06d5b1a7-2769-43f1-858b-73b23bc79fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.2.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports & config**"
      ],
      "metadata": {
        "id": "n0kQN_bwNx3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import fsspec\n",
        "from itertools import islice\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "REPO = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
        "\n",
        "\n",
        "CATEGORIES = [\"Software\", \"Video_Games\", \"All_Beauty\"]\n",
        "ALL_CATEGORIES = [\"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\", \"Automotive\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Books\",\n",
        "              \"CDs_and_Vinyl\", \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewelry\", \"Digital_Music\", \"Electronics\", \"Gift_Cards\", \"Grocery_and_Gourmet_Food\",\n",
        "              \"Handmade_Products\", \"Health_and_Household\", \"Health_and_Personal_Care\", \"Home_and_Kitchen\", \"Industrial_and_Scientific\",\n",
        "              \"Kindle_Store\", \"Magazine_Subscriptions\", \"Movies_and_TV\", \"Musical_Instruments\", \"Office_Products\", \"Patio_Lawn_and_Garden\", \"Pet_Supplies\",\n",
        "              \"Software\", \"Sports_and_Outdoors\", \"Subscription_Boxes\", \"Tools_and_Home_Improvement\", \"Toys_and_Games\", \"Video_Games\",\n",
        "              \"Unknown\"]\n",
        "\n",
        "\n",
        "N_PER_CAT = 10_000\n",
        "N_META    = 60_000\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)"
      ],
      "metadata": {
        "id": "KH6pkdTmN10E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load & sample each category (streaming) and concatenate**"
      ],
      "metadata": {
        "id": "sYW6dqUGeUMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_jsonl(url: str, limit: int | None = None):\n",
        "    \"\"\"\n",
        "    Stream a JSONL file line-by-line from Hugging Face\n",
        "    Normalizes mixed-type fields like 'price'\n",
        "    \"\"\"\n",
        "    with fsspec.open(url, \"rt\") as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            if limit is not None and idx >= limit:\n",
        "                break\n",
        "            obj = json.loads(line)\n",
        "\n",
        "\n",
        "            if \"price\" in obj and obj[\"price\"] is not None:\n",
        "                obj[\"price\"] = str(obj[\"price\"])\n",
        "\n",
        "            return_obj = obj\n",
        "            yield return_obj\n",
        "\n",
        "\n",
        "def ensure_asin(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ensure there is an 'asin' column\n",
        "    \"\"\"\n",
        "    for cand in [\"asin\", \"parent_asin\", \"product_id\", \"item_id\", \"Parent_ASIN\", \"ParentAsin\"]:\n",
        "        if cand in df.columns:\n",
        "            if \"asin\" not in df.columns:\n",
        "                df[\"asin\"] = df[cand]\n",
        "            return df\n",
        "    if len(df) > 0:\n",
        "        print(\"No recognizable ASIN-like key found. Example row:\\n\", df.head(1).to_dict(\"records\")[0])\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_category(category: str, n_reviews: int, n_meta: int):\n",
        "    \"\"\"\n",
        "    Load one category's reviews and meta as DataFrames\n",
        "    \"\"\"\n",
        "    reviews_url = f\"hf://datasets/{REPO}/raw/review_categories/{category}.jsonl\"\n",
        "    meta_url    = f\"hf://datasets/{REPO}/raw/meta_categories/meta_{category}.jsonl\"\n",
        "\n",
        "    reviews_df = pd.DataFrame(islice(stream_jsonl(reviews_url), n_reviews)).assign(category=category)\n",
        "    meta_df    = pd.DataFrame(islice(stream_jsonl(meta_url),    n_meta)).assign(category=category)\n",
        "    return reviews_df, meta_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ywISq46crPoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inspect schemas and key columns**"
      ],
      "metadata": {
        "id": "wMKbJ5SBerZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_reviews, all_meta = [], []\n",
        "\n",
        "for cat in CATEGORIES:\n",
        "    r_df, m_df = load_category(cat, n_reviews=N_PER_CAT, n_meta=N_META)\n",
        "    all_reviews.append(r_df)\n",
        "    all_meta.append(m_df)\n",
        "\n",
        "reviews_df = pd.concat(all_reviews, ignore_index=True)\n",
        "meta_df    = pd.concat(all_meta,    ignore_index=True)\n",
        "\n",
        "reviews_df = ensure_asin(reviews_df)\n",
        "meta_df    = ensure_asin(meta_df)\n",
        "\n",
        "\n",
        "if \"asin\" in reviews_df:\n",
        "    reviews_df = reviews_df[reviews_df[\"asin\"].notna()]\n",
        "if \"asin\" in meta_df:\n",
        "    meta_df = meta_df[meta_df[\"asin\"].notna()]\n",
        "\n",
        "print(f\"Loaded rows -> reviews: {len(reviews_df):,} | meta: {len(meta_df):,}\")\n",
        "display(reviews_df.head(2))\n",
        "display(meta_df.head(2))\n",
        "\n",
        "print(f\"Unique products in reviews: {reviews_df['asin'].nunique():,}\")\n",
        "print(f\"Unique products in meta: {meta_df['asin'].nunique():,}\")\n"
      ],
      "metadata": {
        "id": "zWG-GjTrrMgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(reviews_df.columns)\n",
        "# print(meta_df.columns)\n",
        "# merged = reviews_df.merge(meta_df, on=\"parent_asin\", how=\"left\", suffixes=(\"_review\", \"_meta\"))\n",
        "# print(merged)\n",
        "# print(merged.columns)\n",
        "# merged.shape"
      ],
      "metadata": {
        "id": "ZWfpZGXvwcLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper: ensure_asin + normalize IDs**"
      ],
      "metadata": {
        "id": "RRx0vu7lebCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_keys = {\"asin\", \"parent_asin\", \"category\"}\n",
        "meta_keep = [\"asin\", \"parent_asin\"] + [c for c in meta_df.columns if c not in meta_keys]\n",
        "\n",
        "\n",
        "m1 = reviews_df.merge(meta_df[meta_keep], on=\"asin\", how=\"left\", suffixes=(\"_review\", \"_meta\"))\n",
        "\n",
        "\n",
        "m2 = reviews_df.merge(\n",
        "    meta_df[meta_keep].rename(columns={\"asin\": \"asin_meta2\", \"parent_asin\": \"parent_asin_meta2\"}),\n",
        "    left_on=\"parent_asin\",\n",
        "    right_on=\"asin_meta2\",\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "\n",
        "merged = m1.copy()\n",
        "for col in meta_keep:\n",
        "    if col in {\"asin\", \"parent_asin\"}:\n",
        "        continue\n",
        "    col_m1 = col\n",
        "    col_m2 = col + \"_m2\"\n",
        "    if col in m2.columns:\n",
        "        merged[col_m2] = m2[col]\n",
        "        merged[col] = merged[col].where(merged[col].notna(), merged[col_m2])\n",
        "        merged.drop(columns=[col_m2], inplace=True)\n",
        "\n",
        "\n",
        "if \"asin_meta2\" in m2.columns:\n",
        "    merged[\"asin_meta_fallback\"] = m2[\"asin_meta2\"]\n",
        "\n",
        "print(\"Merged shape:\", merged.shape)\n",
        "\n",
        "\n",
        "meta_signal = [c for c in merged.columns if c.endswith(\"_meta\") or c in [\"average_rating\", \"rating_number\", \"price\", \"store\", \"categories\", \"details\", \"title\", \"images\", \"videos\", \"main_category\"]]\n",
        "coverage = merged[meta_signal].notna().any(axis=1).mean() if meta_signal else 0.0\n",
        "print(f\"Rows with ANY meta fields: {coverage:.2%}\")\n",
        "\n",
        "display(merged.head(5))"
      ],
      "metadata": {
        "id": "tOMpYsb9rYi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Milestone #1: Sentiment Analysis of a Singular Review**\n"
      ],
      "metadata": {
        "id": "Fgmk-O7tD_NL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: Take the reviews dataframe, only maintain the rating, title, category, and text columns, and then train a model that predicts the rating given a review text\n"
      ],
      "metadata": {
        "id": "1RvtkE9vEWtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_category_into_review(category: str, n_reviews: int):\n",
        "    \"\"\"\n",
        "    Load one category's reviews as DataFrames\n",
        "    \"\"\"\n",
        "    reviews_url = f\"hf://datasets/{REPO}/raw/review_categories/{category}.jsonl\"\n",
        "\n",
        "    data = (\n",
        "        {k: row.get(k) for k in [\"rating\", \"title\", \"text\"]}\n",
        "        for row in islice(stream_jsonl(reviews_url), n_reviews)\n",
        "    )\n",
        "\n",
        "    reviews_df = pd.DataFrame(data).assign(category=category)\n",
        "    return reviews_df"
      ],
      "metadata": {
        "id": "Ml7wXwvfEpAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_reviews =  []\n",
        "\n",
        "for cat in ALL_CATEGORIES:\n",
        "    r_df = load_category_into_review(cat, n_reviews=N_PER_CAT)\n",
        "    sentiment_reviews.append(r_df)\n",
        "\n",
        "reviews_df_milestone1 = pd.concat(sentiment_reviews, ignore_index=True)\n",
        "\n",
        "\n",
        "print(f\"Loaded rows -> reviews: {len(reviews_df_milestone1):,}\")\n",
        "display(reviews_df_milestone1.head(2))"
      ],
      "metadata": {
        "id": "x2HCliXaFbUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.info()\n",
        "reviews_df_milestone1['rating'].value_counts()"
      ],
      "metadata": {
        "id": "6r_dO7urHZ78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone #1: Data Cleaning"
      ],
      "metadata": {
        "id": "uHKrG0joIYVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.isna().sum()"
      ],
      "metadata": {
        "id": "-I-iBiBqHajJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Normalization (removing punctuation)"
      ],
      "metadata": {
        "id": "ZZ8x0--IT4qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "\n",
        "def remove_punctuation(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Function removes all punctuation from a string\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))"
      ],
      "metadata": {
        "id": "j30V-EbkT2TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "   Creates clean_review and clean_title and clean_review. These two columns will be used during model training.\n",
        "\"\"\"\n",
        "reviews_df_milestone1['clean_review'] = (\n",
        "    reviews_df_milestone1['text']\n",
        "    .str.lower()\n",
        "    .apply(remove_punctuation)\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "reviews_df_milestone1['clean_title'] = (\n",
        "    reviews_df_milestone1['title']\n",
        "    .str.lower()\n",
        "    .apply(remove_punctuation)\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    .str.strip()\n",
        ")"
      ],
      "metadata": {
        "id": "6meTumTwIeFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmitization of Reviews"
      ],
      "metadata": {
        "id": "Cdv3iTFJbE2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return \" \".join(lemmas)"
      ],
      "metadata": {
        "id": "STEnIP8cbujd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1['lemmatized_review'] = reviews_df_milestone1['clean_review'].apply(lemmatize_text)\n",
        "reviews_df_milestone1['lemmatized_title'] = reviews_df_milestone1['clean_title'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "0EVpjl62cAEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Sentiment Labels\n"
      ],
      "metadata": {
        "id": "1gD-wBOoYFyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentiment_label(rating: int) -> str:\n",
        "  if rating >= 4:\n",
        "    return 'positive'\n",
        "  elif rating <= 2:\n",
        "    return 'negative'\n",
        "  else:\n",
        "    return 'neutral'"
      ],
      "metadata": {
        "id": "hypuTaGqYK3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1['sentiment_labels'] = (\n",
        "    reviews_df_milestone1['rating']\n",
        "    .apply(create_sentiment_label)\n",
        ")"
      ],
      "metadata": {
        "id": "XQ4S2UrnYV-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.head()"
      ],
      "metadata": {
        "id": "-tI5cmBlYeZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization of Reviews\n"
      ],
      "metadata": {
        "id": "nIqjyAdYdxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# documents = reviews_df_milestone1['clean_review'].tolist()"
      ],
      "metadata": {
        "id": "XiY9sb8-dVpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer = TfidfVectorizer(\n",
        "#     stop_words=\"english\",   # remove english stopwords like this, a, the, etc\n",
        "#     # max_features=5000,      # keep top 5000 words (tune this)\n",
        "# )\n",
        "# X = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "eiBi6Z1VdXeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "\n",
        "# df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "# df_tfidf.head()"
      ],
      "metadata": {
        "id": "uxObKv5odZUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "reviews_df_milestone1['tokenized_review'] = reviews_df_milestone1['clean_review'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "PjPm7m-Xh6mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.head(5)"
      ],
      "metadata": {
        "id": "rP5_RHwGiGMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate accelerate scikit-learn torch"
      ],
      "metadata": {
        "id": "HzmENJ6CHtOc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = reviews_df_milestone1.copy()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nUW1qihBqNNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    'negative':0,\n",
        "    'neutral':1,\n",
        "    'positive':2\n",
        "}\n",
        "\n",
        "df['label'] = df['sentiment_labels'].map(label_map)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3lgtqj5VqdNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "EkkVL6E3q0MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "val_ds = Dataset.from_pandas(val_df)\n",
        "test_ds = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "5fZmECPErRkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
      ],
      "metadata": {
        "id": "YYus1C1_rbTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples['clean_review'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "train_ds = train_ds.map(tokenize_function, batched=True)\n",
        "val_ds = val_ds.map(tokenize_function, batched=True)\n",
        "test_ds = test_ds.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "i4gDuyLcriIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "val_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n"
      ],
      "metadata": {
        "id": "iUGjZOIcsJhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels=3)\n"
      ],
      "metadata": {
        "id": "oZGs7loWtIWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install transformers\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "QOflFydduqRp",
        "outputId": "05e94708-ec86-4524-bbf8-bf56d8a915f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.57.1\n",
            "Uninstalling transformers-4.57.1:\n",
            "  Successfully uninstalled transformers-4.57.1\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.57.1\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args= TrainingArguments(\n",
        "    output_dir='./twitter_roberta_results',\n",
        "    evaluation_strategy ='epoch',\n",
        "    save_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    logging_dir='./twitter_roberta_logs',\n",
        "    logging_steps=100\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "q1BhjSr1tq6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j8_Lgy6kufd2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}