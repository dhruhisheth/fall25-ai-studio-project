{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BTT-Cadence-Design-Systems-2A/AI-Studio-Project/blob/Bert-base-multilingual-uncased-sentiment/Cadence_2A_Bert_base_multilingual_uncased_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Install libraries**"
      ],
      "metadata": {
        "id": "a0Py6PRMeBIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XkPWATdBrLTM"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports & config**"
      ],
      "metadata": {
        "id": "n0kQN_bwNx3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import fsspec\n",
        "from itertools import islice\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "REPO = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
        "\n",
        "\n",
        "CATEGORIES = [\"Software\", \"Video_Games\", \"All_Beauty\"]\n",
        "ALL_CATEGORIES = [\"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\", \"Automotive\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Books\",\n",
        "              \"CDs_and_Vinyl\", \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewelry\", \"Digital_Music\", \"Electronics\", \"Gift_Cards\", \"Grocery_and_Gourmet_Food\",\n",
        "              \"Handmade_Products\", \"Health_and_Household\", \"Health_and_Personal_Care\", \"Home_and_Kitchen\", \"Industrial_and_Scientific\",\n",
        "              \"Kindle_Store\", \"Magazine_Subscriptions\", \"Movies_and_TV\", \"Musical_Instruments\", \"Office_Products\", \"Patio_Lawn_and_Garden\", \"Pet_Supplies\",\n",
        "              \"Software\", \"Sports_and_Outdoors\", \"Subscription_Boxes\", \"Tools_and_Home_Improvement\", \"Toys_and_Games\", \"Video_Games\",\n",
        "              \"Unknown\"]\n",
        "\n",
        "\n",
        "N_PER_CAT = 10_000\n",
        "N_META    = 60_000\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)"
      ],
      "metadata": {
        "id": "KH6pkdTmN10E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load & sample each category (streaming) and concatenate**"
      ],
      "metadata": {
        "id": "sYW6dqUGeUMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_jsonl(url: str, limit: int | None = None):\n",
        "    \"\"\"\n",
        "    Stream a JSONL file line-by-line from Hugging Face\n",
        "    Normalizes mixed-type fields like 'price'\n",
        "    \"\"\"\n",
        "    with fsspec.open(url, \"rt\") as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            if limit is not None and idx >= limit:\n",
        "                break\n",
        "            obj = json.loads(line)\n",
        "\n",
        "\n",
        "            if \"price\" in obj and obj[\"price\"] is not None:\n",
        "                obj[\"price\"] = str(obj[\"price\"])\n",
        "\n",
        "            return_obj = obj\n",
        "            yield return_obj\n",
        "\n",
        "\n",
        "def ensure_asin(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ensure there is an 'asin' column\n",
        "    \"\"\"\n",
        "    for cand in [\"asin\", \"parent_asin\", \"product_id\", \"item_id\", \"Parent_ASIN\", \"ParentAsin\"]:\n",
        "        if cand in df.columns:\n",
        "            if \"asin\" not in df.columns:\n",
        "                df[\"asin\"] = df[cand]\n",
        "            return df\n",
        "    if len(df) > 0:\n",
        "        print(\"No recognizable ASIN-like key found. Example row:\\n\", df.head(1).to_dict(\"records\")[0])\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_category(category: str, n_reviews: int, n_meta: int):\n",
        "    \"\"\"\n",
        "    Load one category's reviews and meta as DataFrames\n",
        "    \"\"\"\n",
        "    reviews_url = f\"hf://datasets/{REPO}/raw/review_categories/{category}.jsonl\"\n",
        "    meta_url    = f\"hf://datasets/{REPO}/raw/meta_categories/meta_{category}.jsonl\"\n",
        "\n",
        "    reviews_df = pd.DataFrame(islice(stream_jsonl(reviews_url), n_reviews)).assign(category=category)\n",
        "    meta_df    = pd.DataFrame(islice(stream_jsonl(meta_url),    n_meta)).assign(category=category)\n",
        "    return reviews_df, meta_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ywISq46crPoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inspect schemas and key columns**"
      ],
      "metadata": {
        "id": "wMKbJ5SBerZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_reviews, all_meta = [], []\n",
        "\n",
        "for cat in CATEGORIES:\n",
        "    r_df, m_df = load_category(cat, n_reviews=N_PER_CAT, n_meta=N_META)\n",
        "    all_reviews.append(r_df)\n",
        "    all_meta.append(m_df)\n",
        "\n",
        "reviews_df = pd.concat(all_reviews, ignore_index=True)\n",
        "meta_df    = pd.concat(all_meta,    ignore_index=True)\n",
        "\n",
        "reviews_df = ensure_asin(reviews_df)\n",
        "meta_df    = ensure_asin(meta_df)\n",
        "\n",
        "\n",
        "if \"asin\" in reviews_df:\n",
        "    reviews_df = reviews_df[reviews_df[\"asin\"].notna()]\n",
        "if \"asin\" in meta_df:\n",
        "    meta_df = meta_df[meta_df[\"asin\"].notna()]\n",
        "\n",
        "print(f\"Loaded rows -> reviews: {len(reviews_df):,} | meta: {len(meta_df):,}\")\n",
        "display(reviews_df.head(2))\n",
        "display(meta_df.head(2))\n",
        "\n",
        "print(f\"Unique products in reviews: {reviews_df['asin'].nunique():,}\")\n",
        "print(f\"Unique products in meta: {meta_df['asin'].nunique():,}\")\n"
      ],
      "metadata": {
        "id": "zWG-GjTrrMgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(reviews_df.columns)\n",
        "# print(meta_df.columns)\n",
        "# merged = reviews_df.merge(meta_df, on=\"parent_asin\", how=\"left\", suffixes=(\"_review\", \"_meta\"))\n",
        "# print(merged)\n",
        "# print(merged.columns)\n",
        "# merged.shape"
      ],
      "metadata": {
        "id": "ZWfpZGXvwcLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper: ensure_asin + normalize IDs**"
      ],
      "metadata": {
        "id": "RRx0vu7lebCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_keys = {\"asin\", \"parent_asin\", \"category\"}\n",
        "meta_keep = [\"asin\", \"parent_asin\"] + [c for c in meta_df.columns if c not in meta_keys]\n",
        "\n",
        "\n",
        "m1 = reviews_df.merge(meta_df[meta_keep], on=\"asin\", how=\"left\", suffixes=(\"_review\", \"_meta\"))\n",
        "\n",
        "\n",
        "m2 = reviews_df.merge(\n",
        "    meta_df[meta_keep].rename(columns={\"asin\": \"asin_meta2\", \"parent_asin\": \"parent_asin_meta2\"}),\n",
        "    left_on=\"parent_asin\",\n",
        "    right_on=\"asin_meta2\",\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "\n",
        "merged = m1.copy()\n",
        "for col in meta_keep:\n",
        "    if col in {\"asin\", \"parent_asin\"}:\n",
        "        continue\n",
        "    col_m1 = col\n",
        "    col_m2 = col + \"_m2\"\n",
        "    if col in m2.columns:\n",
        "        merged[col_m2] = m2[col]\n",
        "        merged[col] = merged[col].where(merged[col].notna(), merged[col_m2])\n",
        "        merged.drop(columns=[col_m2], inplace=True)\n",
        "\n",
        "\n",
        "if \"asin_meta2\" in m2.columns:\n",
        "    merged[\"asin_meta_fallback\"] = m2[\"asin_meta2\"]\n",
        "\n",
        "print(\"Merged shape:\", merged.shape)\n",
        "\n",
        "\n",
        "meta_signal = [c for c in merged.columns if c.endswith(\"_meta\") or c in [\"average_rating\", \"rating_number\", \"price\", \"store\", \"categories\", \"details\", \"title\", \"images\", \"videos\", \"main_category\"]]\n",
        "coverage = merged[meta_signal].notna().any(axis=1).mean() if meta_signal else 0.0\n",
        "print(f\"Rows with ANY meta fields: {coverage:.2%}\")\n",
        "\n",
        "display(merged.head(5))"
      ],
      "metadata": {
        "id": "tOMpYsb9rYi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Milestone #1: Sentiment Analysis of a Singular Review**\n"
      ],
      "metadata": {
        "id": "Fgmk-O7tD_NL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: Take the reviews dataframe, only maintain the rating, title, category, and text columns, and then train a model that predicts the rating given a review text\n"
      ],
      "metadata": {
        "id": "1RvtkE9vEWtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_category_into_review(category: str, n_reviews: int):\n",
        "    \"\"\"\n",
        "    Load one category's reviews as DataFrames\n",
        "    \"\"\"\n",
        "    reviews_url = f\"hf://datasets/{REPO}/raw/review_categories/{category}.jsonl\"\n",
        "\n",
        "    data = (\n",
        "        {k: row.get(k) for k in [\"rating\", \"title\", \"text\"]}\n",
        "        for row in islice(stream_jsonl(reviews_url), n_reviews)\n",
        "    )\n",
        "\n",
        "    reviews_df = pd.DataFrame(data).assign(category=category)\n",
        "    return reviews_df"
      ],
      "metadata": {
        "id": "Ml7wXwvfEpAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_reviews =  []\n",
        "\n",
        "for cat in ALL_CATEGORIES:\n",
        "    r_df = load_category_into_review(cat, n_reviews=N_PER_CAT)\n",
        "    sentiment_reviews.append(r_df)\n",
        "\n",
        "reviews_df_milestone1 = pd.concat(sentiment_reviews, ignore_index=True)\n",
        "\n",
        "\n",
        "print(f\"Loaded rows -> reviews: {len(reviews_df_milestone1):,}\")\n",
        "display(reviews_df_milestone1.head(2))"
      ],
      "metadata": {
        "id": "x2HCliXaFbUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.info()\n",
        "reviews_df_milestone1['rating'].value_counts()"
      ],
      "metadata": {
        "id": "6r_dO7urHZ78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone #1: Data Cleaning"
      ],
      "metadata": {
        "id": "uHKrG0joIYVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.isna().sum()"
      ],
      "metadata": {
        "id": "-I-iBiBqHajJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Normalization (removing punctuation)"
      ],
      "metadata": {
        "id": "ZZ8x0--IT4qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "\n",
        "def remove_punctuation(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Function removes all punctuation from a string\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))"
      ],
      "metadata": {
        "id": "j30V-EbkT2TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "   Creates clean_review and clean_title and clean_review. These two columns will be used during model training.\n",
        "\"\"\"\n",
        "reviews_df_milestone1['clean_review'] = (\n",
        "    reviews_df_milestone1['text']\n",
        "    .str.lower()\n",
        "    .apply(remove_punctuation)\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "reviews_df_milestone1['clean_title'] = (\n",
        "    reviews_df_milestone1['title']\n",
        "    .str.lower()\n",
        "    .apply(remove_punctuation)\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    .str.strip()\n",
        ")"
      ],
      "metadata": {
        "id": "6meTumTwIeFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmitization of Reviews"
      ],
      "metadata": {
        "id": "Cdv3iTFJbE2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return \" \".join(lemmas)"
      ],
      "metadata": {
        "id": "STEnIP8cbujd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1['lemmatized_review'] = reviews_df_milestone1['clean_review'].apply(lemmatize_text)\n",
        "reviews_df_milestone1['lemmatized_title'] = reviews_df_milestone1['clean_title'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "0EVpjl62cAEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Sentiment Labels\n"
      ],
      "metadata": {
        "id": "1gD-wBOoYFyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentiment_label(rating: int) -> str:\n",
        "  if rating >= 4:\n",
        "    return 'positive'\n",
        "  elif rating <= 2:\n",
        "    return 'negative'\n",
        "  else:\n",
        "    return 'neutral'"
      ],
      "metadata": {
        "id": "hypuTaGqYK3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1['sentiment_labels'] = (\n",
        "    reviews_df_milestone1['rating']\n",
        "    .apply(create_sentiment_label)\n",
        ")"
      ],
      "metadata": {
        "id": "XQ4S2UrnYV-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.head()"
      ],
      "metadata": {
        "id": "-tI5cmBlYeZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization of Reviews\n"
      ],
      "metadata": {
        "id": "nIqjyAdYdxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# documents = reviews_df_milestone1['clean_review'].tolist()"
      ],
      "metadata": {
        "id": "XiY9sb8-dVpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer = TfidfVectorizer(\n",
        "#     stop_words=\"english\",   # remove english stopwords like this, a, the, etc\n",
        "#     # max_features=5000,      # keep top 5000 words (tune this)\n",
        "# )\n",
        "# X = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "eiBi6Z1VdXeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "\n",
        "# df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "# df_tfidf.head()"
      ],
      "metadata": {
        "id": "uxObKv5odZUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "reviews_df_milestone1['tokenized_review'] = reviews_df_milestone1['clean_review'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "PjPm7m-Xh6mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.head(5)"
      ],
      "metadata": {
        "id": "rP5_RHwGiGMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert-base Multilingual Uncased Model for sentiment analysis"
      ],
      "metadata": {
        "id": "usj5lHibpypQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "zOCsHwR10ZxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "aKKgaHV4j73x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert \"clean review\" column to Dataset"
      ],
      "metadata": {
        "id": "1FMq91W1trFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import Dataset\n",
        "# clean_reviews_dataset = Dataset.from_pandas(reviews_df_milestone1[['clean_review']])"
      ],
      "metadata": {
        "id": "wGaYlTmLt2Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(clean_reviews_dataset)"
      ],
      "metadata": {
        "id": "MvCzmwKduYWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model and model's tokenizer to convert cleaned review text to number embeddings"
      ],
      "metadata": {
        "id": "FZPjl3TZpmAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from os import truncate\n",
        "# from transformers import BertTokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# # define the tokenization function\n",
        "# def tokenize_text(examples):\n",
        "#   return tokenizer(examples['clean_review'], padding=True, truncation=True, max_length=256)\n",
        "\n",
        "# # apply tokenization func to the clean review text\n",
        "# tokenized_clean_reviews = clean_reviews_dataset.map(tokenize_text, batched=True)"
      ],
      "metadata": {
        "id": "E4qLOCWYk0Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tokenized_clean_reviews)"
      ],
      "metadata": {
        "id": "CFfXqjbKphqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the tokenized text into training and validation set"
      ],
      "metadata": {
        "id": "eEaAPZIcq2-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_valid_dataset = tokenized_clean_reviews.train_test_split(test_size=0.2)\n",
        "# train_dataset = train_valid_dataset['train']\n",
        "# valid_dataset = train_valid_dataset['test']"
      ],
      "metadata": {
        "id": "4-PDr9u8qpAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_dataset)\n",
        "# print(f\"train dataset shape: {train_dataset.shape}\")\n",
        "# print(valid_dataset)\n",
        "# print(f\"valid dataset shape: {valid_dataset.shape}\")"
      ],
      "metadata": {
        "id": "KWj52UONzTjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create data loader to manage batches of data during training\n",
        "Dataloader is used to organize data for model training by providing efficient ways to batch, shuffle, and transform data.\n"
      ],
      "metadata": {
        "id": "0e_YAVegzunV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
        "# valid_dataloader = DataLoader(valid_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "ECYRGIpVz3Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the model and config for fine-tuning the model\n",
        "\n",
        "AdamW is for adjusting learning rate during training.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Igszu8F1U2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "# from torch.optim import AdamW\n",
        "\n",
        "# # load the pre-trained bert model for sequence classification\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3) # num_labels will be 3 (we are classifying positive, negative, or neutral)"
      ],
      "metadata": {
        "id": "HlabyXAk1gkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # define training argument\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results', # ouput directory for res\n",
        "#     eval_strategy='epoch', # evaluation strategy\n",
        "#     num_train_epochs=3, # num of training epochs\n",
        "#     learning_rate=2e-5, # learning rate\n",
        "#     per_device_train_batch_size=8, # batch size for training\n",
        "#     per_device_eval_batch_size=8, # batch size for evaluation\n",
        "#     weight_decay=0.01 # weight decay\n",
        "\n",
        "# )"
      ],
      "metadata": {
        "id": "LOlL5bIk25V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # define Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=valid_dataset\n",
        "# )"
      ],
      "metadata": {
        "id": "qBmVohGV4r8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()"
      ],
      "metadata": {
        "id": "3wLDhgpcEEfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unsupervised Learning (Clustering) with paraphrase-multilingual-MiniLM-L12-v2"
      ],
      "metadata": {
        "id": "o2IFeuKfZO-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch, numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "KD5ygQf1ZOcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "# model = AutoModel.from_pretrained(\"bert-base-multilingual-uncased\")"
      ],
      "metadata": {
        "id": "kHoeZ8p3Zqvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = reviews_df_milestone1['clean_review'].tolist()\n",
        "data_emb = model.encode(data)"
      ],
      "metadata": {
        "id": "-n2EoBkxouUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_emb)\n",
        "print(data_emb.shape)"
      ],
      "metadata": {
        "id": "Zyi3UWpopl75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune the model"
      ],
      "metadata": {
        "id": "1eAzhcqdpryp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = 2 # positive, negative\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(data_emb)\n",
        "cluster_assignment = clustering_model.labels_"
      ],
      "metadata": {
        "id": "plc4272OZu9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1['model predictions'] = cluster_assignment"
      ],
      "metadata": {
        "id": "Gv2dlq0mpzP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.head(15)"
      ],
      "metadata": {
        "id": "_FK4dUqVr8ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking clusters\n",
        "Although we do 3 clusters, it doesn't directly mean positive, negative, neutral. It just pull similar thing closer"
      ],
      "metadata": {
        "id": "AwI4hMVIzgT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for c in range(3):\n",
        "  print(f\"Cluster {c}\")\n",
        "  print(reviews_df_milestone1[reviews_df_milestone1['model predictions']==c][\"clean_review\"].head(5).tolist())"
      ],
      "metadata": {
        "id": "lrw8w3gYzx7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity between sentiment label embeddings and clean review text embeddings\n",
        "\n",
        "For this, we don't need training. We just need to use paraphrase-multilingual-MiniLM-L12-v2 encoder to encode the embeddings and calculate cosine similarity between them to determine what is the most similar one."
      ],
      "metadata": {
        "id": "9AQrJh7i95HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels = ['positive', 'negative', 'neutral']\n",
        "# label_emb = model.encode(labels)\n",
        "POS = [\n",
        "  \"This review is positive.\", \"I loved it.\", \"excellent, satisfied, would recommend\",\n",
        "  \"great quality\", \"works perfectly\"\n",
        "]\n",
        "NEU = [\n",
        "  \"This review is neutral.\", \"it is okay\", \"average, acceptable\",\n",
        "  \"neither good nor bad\"\n",
        "]\n",
        "NEG = [\n",
        "  \"This review is negative.\", \"I hated it.\", \"terrible, disappointed, refund\",\n",
        "  \"poor quality\", \"does not work\"\n",
        "]\n",
        "\n",
        "def proto_embed(texts):\n",
        "    vecs = model.encode(texts)\n",
        "    return np.mean(vecs, axis=0) # Calculate the mean of the embeddings\n",
        "\n",
        "p_pos = proto_embed(POS)\n",
        "p_neu = proto_embed(NEU)\n",
        "p_neg = proto_embed(NEG)\n",
        "\n",
        "# Stack the mean embeddings\n",
        "protos = np.stack([p_pos, p_neu, p_neg])  # shape: [3, d]"
      ],
      "metadata": {
        "id": "22iHwcsC56_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(label_emb)\n",
        "scores = data_emb @ protos.T                                  # [N, 3]\n",
        "labels = np.array([\"positive\",\"neutral\",\"negative\"])\n",
        "pred = labels[scores.argmax(axis=1)]\n",
        "reviews_df_milestone1[\"sentiment_pred\"] = pred"
      ],
      "metadata": {
        "id": "aL8nbnzG6332"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df_milestone1.head(15)"
      ],
      "metadata": {
        "id": "IILmVRKj7khb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model performance compared with rating labels"
      ],
      "metadata": {
        "id": "c-cIUzhT81w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_true = reviews_df_milestone1['sentiment_labels']\n",
        "y_pred = reviews_df_milestone1['sentiment_pred']\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "classification_report = classification_report(y_true, y_pred)\n",
        "print(f\"Classification Report: {classification_report}\")\n",
        "\n",
        "confusion_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(f\"Confusion Matrix: {confusion_matrix}\")\n"
      ],
      "metadata": {
        "id": "Icjay8t8891g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zitpuT4W9X8j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}